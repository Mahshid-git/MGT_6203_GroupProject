---
title: "Untitled"
author: "MJ"
date: "02/04/2022"
output: html_document
---
# This part mainly examines feature selection of the most relevant predictor variables. Different methods are used including remove multicollinearity, linear regression to remove insignificant variables, Lasso


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# Remove previous data from environment
rm(list = ls())
```

```{r}
library(arrow) # needed to read parquet file
library(dplyr)
library(car)
library(caret)
library(ggplot2)
library(corrplot)
```

```{r}
file_path <- "../02_CleanData/policy_clean.parquet"
df <- read_parquet(file_path, as_tibble = TRUE)
head(df)

```

```{r}
str(df)
```
```{r}
# Select continuous variables for correlation examination
# comment
df_num <- subset(df, select = c(basementenclosurecrawlspacetype, censustract,crsdiscount,elevationdifference,federalpolicyfee, latitude, longitude, policycost, policycount, reportedzipcode, ratemethod, totalbuildinginsurancecoverage, totalcontentsinsurancecoverage))

```


```{r}
# Examine correlation between all numeric variables

png(height=1200, width=1200, pointsize=25, file="overlap.png")
col1 <-rainbow(100, s = 1, v = 1, start = 0, end = 0.9, alpha = 1)

test <- matrix(data=df_num,nrow=13,ncol=13)

corrplot(test,tl.cex=3,title="Overlaps Between methods",
method="circle",is.corr=FALSE,type="full",
cl.lim=c(10,100),cl.cex=2,addgrid.col=
"red",addshade="positive",col=col1, addCoef.col = rgb(0,0,0, alpha =
0.6), mar=c(0,0,1,0), diag= FALSE) 
dev.off()

```
```{r}
cormat <-cor(df_num, use = "pairwise.complete.obs")

corrplot(cormat)
```




```{r}
# Looking for Spearman correlation (checks nonlinear correlation too eg y=x3 ha Spearman correlation of 1)
cormat_nonlin <- cor(df_num, use = "pairwise.complete.obs", method="spearman")
cormat_nonlin
```

```{r}
corrplot(cormat_nonlin)
```

## check Multicollinearity


```{r}
# Multicollinearity
# Check the variables of highly correlated

highly_corelated = findCorrelation(cormat, cutoff = 0.6)

highlyCor_Col = colnames(df_num)[highly_corelated]
highlyCor_Col

```

```{r}
# Now, we remove highly correlated variables from original dataset and create cleaner dataset

df_clean = df[, -which(colnames(df) %in% highlyCor_Col)]
dim(df_clean)

```
```{r}
fit = lm(totalinsurancepremiumofthepolicy ~., df_clean)
summary(fit)
```
## from the output, we can remove some insignificant variables, but keep lattitude and reportedzipcode for mapping the areas where flood occurs

```{r}
fit1 = lm(totalinsurancepremiumofthepolicy ~. -censustract -policytermindicator -upperlevelcondo -mobilehomeortrailer -floodzone_modrisk -floodzone_undetermined, df_clean)
summary(fit1)
```

## Create the final dataset for further analysis

```{r}
df_final <- subset(df_clean, select = -c(censustract,policytermindicator,mobilehomeortrailer, upperlevelcondo,floodzone_modrisk,floodzone_undetermined))

View(df_final)
```

```{r}
str(df_final)
```


```{r}
# Select only numerical columns for data analysist (leave data type in df_final)
# df_num inlcudes 25 variables

df_num <- subset(df_final, select = -c(originalconstructiondate, originalnbdate, policyeffectivedate, policyterminationdate))

View(df_num)
```

```{r}
## Variable selections using Lasso

train_con <- trainControl(method = 'cv', number = 5) # using 5 folds cv

lasso <- train(totalinsurancepremiumofthepolicy ~., df_num,
               method = 'lasso',
               preProc = c('scale', 'center'),
               trControl = train_con,
              na.action=na.exclude)
lasso
```

```{r}
# Get coef

predict(lasso$finalModel, type = "coef", mode = "fraction", s = as.numeric(lasso$bestTune))

# We can further discard variables with coefficients reduced to zero
```

# From the Lasso Regression models, we can discard the variables with coeeficients near to zero, so the totalinsurancepremiumofthepolicy will mainly depends on 7 variables as folows: totalinsurancepremiumofthepolicy ~ deductibleamountincontentscoverage, federalpolicyfee, totalbuildinginsurancecoverage, totalcontentsinsurancecoverage, primaryresidenceindicator, policycost, floodzone_highrisk 



```{r}

# Exporting the dataframe to clean folder

# df_final includes "date" columns

file_path3 <- "../03_EDA/df_final.parquet"
write_parquet(df_final, file_path3)

# df_num includes only numeric variables for analysis
file_path4 <- "../03_EDA/df_num.parquet"
write_parquet(df_num, file_path4)

# df includes only 7 predicted variables and response as suggested by Lasso for analysis

df <- subset(df_num, select = c(totalinsurancepremiumofthepolicy, deductibleamountincontentscoverage, federalpolicyfee, totalbuildinginsurancecoverage, totalcontentsinsurancecoverage, primaryresidenceindicator, policycost, floodzone_highrisk))
                                       
file_path5 <- "../03_EDA/df.parquet"
write_parquet(df, file_path5)

```

## Move to the next section of modeling

```{r}
file_path <- "../03_EDA/df.parquet"
df <- read_parquet(file_path, as_tibble = TRUE)
View(df)
```

```{r}
install_github("ujjwalkarn/xda")
library(xda)
Plot(df,'totalinsurancepremiumofthepolicy')
```




