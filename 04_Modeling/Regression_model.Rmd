---
title: "Untitled"
author: "Anh Tran"
date: "4/3/2022"
output: html_document
---
# This part will involve all Linear Regression models

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# Remove previous data from environment
rm(list = ls())
```

```{r include=FALSE}
if (!require("sjPlot")) install.packages("sjPlot")
if (!require("tidyverse")) install.packages("sjPlot")

library(arrow) # needed to read parquet file

library(dplyr)
library(car)
library(caret)
library(rattle)
library(rpart.plot)
library(rpart)
library(ggplot2)
library(corrplot)
library(sjPlot)
library(gridExtra)
library(tidyverse)
library(rsample)
library(randomForest)
library(janitor)
library(glmnet)
library(tree)
library(RColorBrewer)
library(gridExtra)
library(pls)
```


```{r}
file_path <- "../03_EDA/df_final_0impute_5zipcode.parquet"
df1 <- read_parquet(file_path, as_tibble = TRUE)
#View(df1)
```


## Scale all numerical variables, remove reportedzipcode as we concetrated in 5 top zipcode and thus zipcode is not in need anymore. The dataset includes 293,794 rows of 23 variables and response. Noted that the  target variable is now called totalinsurancepremiumofthepolicy_log_log, as it is the log transformation of the actual variable from previous step.   

```{r}
df = scale(as.data.frame(df1[,c(2,4,5,7,8,9,10,11,14)])) # standardize all numerical variables

df <- cbind(df1[,c(3,6,12,13,15,16,17,19, 20,21,22,23)],df,df1[,1]) # Add categorical columns  back in

#View(df)
```

# LINEAR REGRESSION MODELS

# Splitting data to 70% training, 30% testing

```{r}
set.seed(100)

size <- floor(0.7*nrow(df))
train_ind <- sample(seq_len(nrow(df)), size = size)

train <- df[train_ind, ]
test <- df[-train_ind, ]
```

# Function to calculate R square and MSE
```{r}

rsquared <- function(pred){
  if (length(pred)==length(test$totalinsurancepremiumofthepolicy_log)){
    r2 = 1 - (sum((test$totalinsurancepremiumofthepolicy_log-pred)^2)/sum((test$totalinsurancepremiumofthepolicy_log-mean(test$totalinsurancepremiumofthepolicy_log))^2))
  }
  if (length(pred)==length(train$totalinsurancepremiumofthepolicy_log)){
    r2 = 1 - (sum((train$totalinsurancepremiumofthepolicy_log-pred)^2)/sum((train$totalinsurancepremiumofthepolicy_log-mean(train$totalinsurancepremiumofthepolicy_log))^2))
  }
  return (r2)
}


MSE <- function(pred){
  if (length(pred)==length(test$totalinsurancepremiumofthepolicy_log)){
    mse = sum((test$totalinsurancepremiumofthepolicy_log-pred)^2)/length(test$totalinsurancepremiumofthepolicy_log)
  }
  if (length(pred)==length(train$totalinsurancepremiumofthepolicy_log)){
    mse = sum((train$totalinsurancepremiumofthepolicy_log-pred)^2)/length(train$totalinsurancepremiumofthepolicy_log)
  }
  return (mse)
}
```
 
# LINEAR STEPWISE REGRESSION: forward, backward and both directions

 # 1. FORWARD DIRECTION
 
```{r}
linear.fwd <- step(lm(totalinsurancepremiumofthepolicy_log ~., data = train), direction = c("forward"))

fwd.pred.train = predict(linear.fwd, train)
mse.fwd.train = MSE(fwd.pred.train)
r2.fwd.train = rsquared(fwd.pred.train)

fwd.pred.test = predict(linear.fwd, test)
mse.fwd.test = MSE(fwd.pred.test)
r2.fwd.test = rsquared(fwd.pred.test)

print(paste('R_squared of test data in Forward Selection is', r2.fwd.test))
```
 
 # 2. BACKWARD DIRECTION
 
```{r}
linear.bck <- step(lm(totalinsurancepremiumofthepolicy_log ~., data = train), direction = c("backward"))

bck.pred.train = predict(linear.bck, train)
mse.bck.train = MSE(bck.pred.train)
r2.bck.train = rsquared(bck.pred.train)

bck.pred.test = predict(linear.bck, test)
mse.bck.test = MSE(bck.pred.test)
r2.bck.test = rsquared(bck.pred.test)

print(paste('R_squared of test data in Forward Selection is', r2.bck.test))
```
 
 #3. BOTH DIRECTION
 
```{r}
linear.both <-lm(totalinsurancepremiumofthepolicy_log ~., data = train)

step(linear.both, scope = list(lower = formula(lm(totalinsurancepremiumofthepolicy_log~1, data = train)), 
                              upper = formula(lm(totalinsurancepremiumofthepolicy_log~., data = train))), direction =  "both")

both.pred.train = predict(linear.both, train)
mse.both.train = MSE(both.pred.train)
r2.both.train = rsquared(both.pred.train)

both.pred.test = predict(linear.both, test)
mse.both.test = MSE(both.pred.test)
r2.both.test = rsquared(both.pred.test)

print(paste('R_squared of test data in Both Selection is', r2.both.test))
```






















#1. Linear Regression: Ridge and Lasso












## 1. Ridge
```{r}
xtrain = model.matrix(totalinsurancepremiumofthepolicy_log ~.,train)[,-1]
ytrain = train$totalinsurancepremiumofthepolicy_log

xtest = model.matrix(totalinsurancepremiumofthepolicy_log ~.,test)[,-1]
ytest = test$totalinsurancepremiumofthepolicy_log
```



```{r}
linear.fwd <- step(lm(totalinsurancepremiumofthepolicy_log ~., data= train), direction = c("forward"))

fwd.pred.train = predict(linear.fwd, med.train)
mse.fwd.train = MSE(fwd.pred.train)
r2.fwd.train = rsquared(fwd.pred.train)

fwd.pred.test = predict(linear.fwd, med.test)
mse.fwd.test = MSE(fwd.pred.test)
r2.fwd.test = rsquared(fwd.pred.test)

summary(linear.fwd)
```









```{r}
grid = 10^seq(10, -2, length = 100)

cv.ridge.out = cv.glmnet(xtrain, ytrain, alpha=0)
bestlam.ridge = cv.ridge.out$lambda.min
i <- which(cv.ridge.out$lambda == cv.ridge.out$lambda.min)
mse.min.ridge <- cv.ridge.out$cvm[i]

ridge.model = glmnet(xtrain, ytrain, alpha=0, lambda=bestlam.ridge)

ridge.pred.train = predict(ridge.model, newx=xtrain)
mse.ridge.train = MSE(ridge.pred.train)
r2.ridge.train = rsquared(ridge.pred.train)

ridge.pred.test = predict(ridge.model, newx=xtest)
mse.ridge.test = MSE(ridge.pred.test)
r2.ridge.test = rsquared(ridge.pred.test)
```


## 2. Lasso
```{r}

cv.lasso.out = cv.glmnet(xtrain, ytrain, family = "gaussian", alpha = 1, nfolds = 5, type.measure = "mse")
bestlam.lasso = cv.lasso.out$lambda.min
i <- which(cv.lasso.out$lambda == cv.lasso.out$lambda.min)
mse.min.lasso <- cv.lasso.out$cvm[i]

lasso.model = glmnet(xtrain, ytrain, alpha=1, lambda=bestlam.lasso)

lasso.pred.train = predict(lasso.model, newx=xtrain)
mse.lasso.train = MSE(lasso.pred.train)
r2.lasso.train = rsquared(lasso.pred.train)
r2.lasso.train 

lasso.pred.test = predict(lasso.model, newx=xtest)
mse.lasso.test = MSE(lasso.pred.test)
r2.lasso.test = rsquared(lasso.pred.test)
r2.lasso.test
```

```{r}
ggplot(mapping = aes(x=log(cv.ridge.out$lambda), y=cv.ridge.out$cvm)) +
  geom_point() +
  geom_point(aes(x=log(bestlam.ridge), y=mse.min.ridge, color="blue", size = 2), show.legend = FALSE, color="blue") +
  geom_errorbar(aes(ymin=cv.ridge.out$cvm-cv.ridge.out$cvsd, ymax=cv.ridge.out$cvm+cv.ridge.out$cvsd), color="gray") +
  xlab("Log(lambda)") +
  ylab("Mean Squared Error") +
  labs(title = "Optimal Lambda for Ridge Regression", subtitle = paste("Best Lambda: ", bestlam.ridge)) +
  theme_classic()
dev.copy(jpeg, filename="Ridge_Lambda.jpg");
dev.off ()



ggplot(mapping = aes(x=log(cv.lasso.out$lambda), y=cv.lasso.out$cvm)) +
  geom_point() +
  geom_point(aes(x=log(bestlam.lasso), y=mse.min.lasso, size = 2), show.legend = FALSE, color="red") +
  geom_errorbar(aes(ymin=cv.lasso.out$cvm-cv.lasso.out$cvsd, ymax=cv.lasso.out$cvm+cv.lasso.out$cvsd), color="gray") +
  xlab("Log(lambda)") +
  ylab("Mean Squared Error") +
  labs(title = "Optimal Lambda for Lasso Regression", subtitle = paste("Best Lambda: ", bestlam.lasso)) +
  theme_classic()
dev.copy(jpeg, filename="Lasso_Lambda.jpg");
dev.off ()
```


```{r}
ridge.coef <- rownames_to_column(data.frame(coef(ridge.model)[,1]), var = "Variable") %>%
  rename(Coefficient = coef.ridge.model....1.)

ridge.coef <- ridge.coef %>%
  filter(Variable != "(Intercept)") %>%
  arrange(desc(abs(Coefficient)))
lasso.coef <- rownames_to_column(data.frame(coef(lasso.model)[,1]), var = "Variable") %>%
  rename(Coefficient = coef.lasso.model....1.)

lasso.coef <- lasso.coef %>%
  filter(Variable != "(Intercept)") %>%
  arrange(desc(abs(Coefficient)))

coef.compare <- lasso.coef %>%
  left_join(ridge.coef, by = "Variable") %>%
  rename("Lasso Coefficient" = Coefficient.x) %>%
  rename("Ridge Coefficient" = Coefficient.y)

coef.compare
write.csv(coef.compare,"../04_Modeling/Ridge_Lasso_Lambda", row.names = FALSE)
```

```{r}
plot1 <- ggplot(mapping = aes(x = test$totalinsurancepremiumofthepolicy_log, y = ridge.pred.test)) +
  geom_point(color = "red") +
  geom_abline(slope = 1) +
  xlab("Predicted Insurance Premium") +
  ylab("Actual Insurance Premium") +
  labs(title = "Ridge Regression")

plot2 <- ggplot(mapping = aes(x = test$totalinsurancepremiumofthepolicy_log, y = lasso.pred.test)) +
  geom_point(color = "blue") +
  geom_abline(slope = 1) +
  xlab("Predicted Insurance Premium") +
  ylab("Actual Insurance Premium") +
  labs(title = "Lasso")

grid.arrange(plot1, plot2, nrow = 2)

dev.copy(jpeg, filename="Ridge_Lasso_Plot.jpg")
dev.off ()
```





# Bagging 
```{r}
set.seed(100)

bag.model <- randomForest(totalinsurancepremiumofthepolicy_log ~., data = train, mtry = 5, importance = TRUE)

pred.bag.train <- predict(bag.model, train)
mse.bag.train <- MSE(pred.bag.train)
r2.bag.train <- rsquared(pred.bag.train)

pred.bag.test <- predict(bag.model, test)
mse.bag.test <- MSE(pred.bag.test)
r2.bag.test <- rsquared(pred.bag.test)
```








#2. PCA

```{r}
#PCA
library(stats)  # for prcomp()
library(DAAG)
library(GGally)
library(devtools)
require(ggbiplot)
```


```{r}
PCA <- prcomp(df[,1:22])  # apply PCA

summary(PCA)
```


```{r}
# Common function to display PCA related plots in 2x2 grid
screeplot(PCA, type='l')
dev.copy(jpeg, filename="PCA_ScreePlot.jpg")
dev.off ()

bplot = ggbiplot(PCA,
                 choices = c(1,2),
                 obs.scale =1, var.scale =1,
                 circle = TRUE,
                 ellipse = TRUE,
                 ellipse.prob = 0.68)

print(bplot)

dev.copy(jpeg, filename="PCA_Plot.jpg")
dev.off ()

```
```{r}
View(df)
PCA$x[,1:8]
#PCA_7 <- cbind(df[,23], PCA$x[,1:8])
#head(pd.DataFrame(PCA_7))
```



```{r}
PCA_7 <- cbind(df[,23], PCA$x[,1:8])
modelPCA <- lm(V1~.,data = as.data.frame(PCA_7)) # fit model
summary(modelPCA) 

cross_valPCA <- cv.lm(as.data.frame(PCA_7), modelPCA, m=5) # cross-validate 
R2_PCA<- 1 - attr(cross_valPCA,"ms")*nrow(df)/sum((df$totalinsurancepremiumofthepolicy_log - mean(df$totalinsurancepremiumofthepolicy_log))^2) # calculate R-squared
R2_PCA
```





```{r}
p1 <- ggplot(mapping = aes(x =test$totalinsurancepremiumofthepolicy_log, y = pred.bag.test)) +
  geom_point(color = "#FFB5C5") +
  geom_abline(slope = 1) +
  xlab("Predicted Premium Price") +
  ylab("Actual Premium Price") +
  labs(title = "Bagging")




p3 <- ggplot(mapping = aes(x = test$totalinsurancepremiumofthepolicy_log, y = lasso.pred.test)) +
  geom_point(color = "steelblue4") +
  geom_abline(slope = 1) +
  xlab("Predicted Premium Price") +
  ylab("Actual Premium Price") +
  labs(title = "Lasso")



grid.arrange(p1,  p3, nrow = 2)
```
















