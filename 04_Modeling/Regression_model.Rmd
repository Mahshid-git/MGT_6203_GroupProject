---
title: "Untitled"
author: "Anh Tran"
date: "4/3/2022"
output: html_document
---
# This part will involve all Linear Regression models

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# Remove previous data from environment
rm(list = ls())
```

```{r include=FALSE}
if (!require("sjPlot")) install.packages("sjPlot")
if (!require("tidyverse")) install.packages("sjPlot")

library(arrow) # needed to read parquet file

library(dplyr)
library(car)
library(caret)
library(rattle)
library(rpart.plot)
library(rpart)
library(ggplot2)
library(corrplot)
library(sjPlot)
library(gridExtra)
library(tidyverse)
library(rsample)
library(randomForest)
library(janitor)
library(glmnet)
library(tree)
library(RColorBrewer)
library(gridExtra)
library(pls)
```


```{r}
file_path <- "../03_EDA/df_final_0impute_5zipcode.parquet"
df1 <- read_parquet(file_path, as_tibble = TRUE)
#View(df1)
```


## Scale all numerical variables, remove reportedzipcode as we concetrated in 5 top zipcode and thus zipcode is not in need anymore. The dataset includes 293,794 rows of 23 variables and response. Noted that the  target variable is now called totalinsurancepremiumofthepolicy_log_log, as it is the log transformation of the actual variable from previous step.   

```{r}
df = scale(as.data.frame(df1[,c(2,4,5,7,8,9,10,11,14)])) # standardize all numerical variables

df <- cbind(df1[,c(3,6,12,13,15,16,17,19, 20,21,22,23)],df,df1[,1]) # Add categorical columns  back in

#View(df)
```

# LINEAR REGRESSION MODELS

# Splitting data to 70% training, 30% testing

```{r}
set.seed(100)

size <- floor(0.7*nrow(df))
train_ind <- sample(seq_len(nrow(df)), size = size)

train <- df[train_ind, ]
test <- df[-train_ind, ]
```


```{r}
xtrain = model.matrix(totalinsurancepremiumofthepolicy_log ~., train)[,-1]
ytrain = train$totalinsurancepremiumofthepolicy_log
xtest = model.matrix(totalinsurancepremiumofthepolicy_log ~., test)[,-1]
ytest = test$totalinsurancepremiumofthepolicy_log
```

# Function to calculate R square and MSE
```{r}

rsquared <- function(pred){
  if (length(pred)==length(test$totalinsurancepremiumofthepolicy_log)){
    r2 = 1 - (sum((test$totalinsurancepremiumofthepolicy_log-pred)^2)/sum((test$totalinsurancepremiumofthepolicy_log-mean(test$totalinsurancepremiumofthepolicy_log))^2))
  }
  if (length(pred)==length(train$totalinsurancepremiumofthepolicy_log)){
    r2 = 1 - (sum((train$totalinsurancepremiumofthepolicy_log-pred)^2)/sum((train$totalinsurancepremiumofthepolicy_log-mean(train$totalinsurancepremiumofthepolicy_log))^2))
  }
  return (r2)
}


MSE <- function(pred){
  if (length(pred)==length(test$totalinsurancepremiumofthepolicy_log)){
    mse = sum((test$totalinsurancepremiumofthepolicy_log-pred)^2)/length(test$totalinsurancepremiumofthepolicy_log)
  }
  if (length(pred)==length(train$totalinsurancepremiumofthepolicy_log)){
    mse = sum((train$totalinsurancepremiumofthepolicy_log-pred)^2)/length(train$totalinsurancepremiumofthepolicy_log)
  }
  return (mse)
}
```
 
# LINEAR STEPWISE REGRESSION: forward, backward and both directions

 # 1. FORWARD DIRECTION
 
```{r}
linear.fwd <- step(lm(totalinsurancepremiumofthepolicy_log ~., data = train), direction = c("forward"))

fwd.pred.train = predict(linear.fwd, train)
mse.fwd.train = MSE(fwd.pred.train)
r2.fwd.train = rsquared(fwd.pred.train)

fwd.pred.test = predict(linear.fwd, test)
mse.fwd.test = MSE(fwd.pred.test)
r2.fwd.test = rsquared(fwd.pred.test)

print(paste('R_squared of test data in Forward Selection is', r2.fwd.test))
print(mse.fwd.test)
```
 
 #2. BOTH DIRECTION
 
```{r}
linear.both <-lm(totalinsurancepremiumofthepolicy_log ~., data = train)

step(linear.both, scope = list(lower = formula(lm(totalinsurancepremiumofthepolicy_log ~ 1, data = train)), 
                              upper = formula(lm(totalinsurancepremiumofthepolicy_log ~., data = train))), direction =  "both")

both.pred.train = predict(linear.both, train)
mse.both.train = MSE(both.pred.train)
r2.both.train = rsquared(both.pred.train)

both.pred.test = predict(linear.both, test)
mse.both.test = MSE(both.pred.test)
r2.both.test = rsquared(both.pred.test)

print(paste('R_squared of test data in Both Selection is', r2.both.test))
#print(mse.both.test)
```

# PENALIZED LINEAR REGRESSION: Ridge and Lasso

#1. Ridge

```{r}
grid = 10^seq(10, -2, length = 100)

cv.ridge.out = cv.glmnet(xtrain, ytrain, alpha=0)
bestlam.ridge = cv.ridge.out$lambda.min
i <- which(cv.ridge.out$lambda == cv.ridge.out$lambda.min)
mse.min.ridge <- cv.ridge.out$cvm[i]

ridge.model = glmnet(xtrain, ytrain, alpha=0, lambda=bestlam.ridge)

ridge.pred.train = predict(ridge.model, newx=xtrain)
mse.ridge.train = MSE(ridge.pred.train)
r2.ridge.train = rsquared(ridge.pred.train)

ridge.pred.test = predict(ridge.model, newx=xtest)
mse.ridge.test = MSE(ridge.pred.test)
r2.ridge.test = rsquared(ridge.pred.test)

print(paste('R_squared of test data in Ridge is', r2.ridge.test))

```

# 2. Lasso

```{r}

cv.lasso.out = cv.glmnet(xtrain, ytrain, family = "gaussian", alpha = 1, nfolds = 5, type.measure = "mse")
bestlam.lasso = cv.lasso.out$lambda.min
i <- which(cv.lasso.out$lambda == cv.lasso.out$lambda.min)
mse.min.lasso <- cv.lasso.out$cvm[i]

lasso.model = glmnet(xtrain, ytrain, alpha=1, lambda=bestlam.lasso)

lasso.pred.train = predict(lasso.model, newx=xtrain)
mse.lasso.train = MSE(lasso.pred.train)
r2.lasso.train = rsquared(lasso.pred.train)
r2.lasso.train 

lasso.pred.test = predict(lasso.model, newx=xtest)
mse.lasso.test = MSE(lasso.pred.test)
r2.lasso.test = rsquared(lasso.pred.test)
r2.lasso.test

print(paste('R_squared of test data in Lasso is', r2.lasso.test))

```

```{r}
ggplot(mapping = aes(x=log(cv.ridge.out$lambda), y=cv.ridge.out$cvm)) +
  geom_point() +
  geom_point(aes(x=log(bestlam.ridge), y=mse.min.ridge, color="blue", size = 2), show.legend = FALSE, color="blue") +
  geom_errorbar(aes(ymin=cv.ridge.out$cvm-cv.ridge.out$cvsd, ymax=cv.ridge.out$cvm+cv.ridge.out$cvsd), color="gray") +
  xlab("Log(lambda)") +
  ylab("Mean Squared Error") +
  labs(title = "Optimal Lambda for Ridge Regression", subtitle = paste("Best Lambda: ", bestlam.ridge)) +
  theme_classic()
dev.copy(jpeg, filename="Ridge_Lambda.jpg");
dev.off ()


ggplot(mapping = aes(x=log(cv.lasso.out$lambda), y=cv.lasso.out$cvm)) +
  geom_point() +
  geom_point(aes(x=log(bestlam.lasso), y=mse.min.lasso, size = 2), show.legend = FALSE, color="red") +
  geom_errorbar(aes(ymin=cv.lasso.out$cvm-cv.lasso.out$cvsd, ymax=cv.lasso.out$cvm+cv.lasso.out$cvsd), color="gray") +
  xlab("Log(lambda)") +
  ylab("Mean Squared Error") +
  labs(title = "Optimal Lambda for Lasso Regression", subtitle = paste("Best Lambda: ", bestlam.lasso)) +
  theme_classic()
dev.copy(jpeg, filename="Lasso_Lambda.jpg");
dev.off ()
```


```{r}
ridge.coef <- rownames_to_column(data.frame(coef(ridge.model)[,1]), var = "Variable") %>%
  rename(Coefficient = coef.ridge.model....1.)

ridge.coef <- ridge.coef %>%
  filter(Variable != "(Intercept)") %>%
  arrange(desc(abs(Coefficient)))
lasso.coef <- rownames_to_column(data.frame(coef(lasso.model)[,1]), var = "Variable") %>%
  rename(Coefficient = coef.lasso.model....1.)

lasso.coef <- lasso.coef %>%
  filter(Variable != "(Intercept)") %>%
  arrange(desc(abs(Coefficient)))

coef.compare <- lasso.coef %>%
  left_join(ridge.coef, by = "Variable") %>%
  rename("Lasso Coefficient" = Coefficient.x) %>%
  rename("Ridge Coefficient" = Coefficient.y)

coef.compare

write.csv(coef.compare,"../04_Modeling/Ridge_Lasso_Lambda", row.names = FALSE)
```
#1.REGRESIION TREE : 

```{r}
treet.model <- rpart(totalinsurancepremiumofthepolicy_log ~., data = train, method = "anova")
rpart.plot(treet.model, main = "Prediction of Total Insurance Premium", extra = 101, digits = -1, yesno = 2, type = 5)

dev.copy(jpeg, filename = "Regression Tree.jpg")
dev.off ()
```

```{r}
pred.tree.train <- predict(treet.model, train)
mse.tree.train <- MSE(pred.tree.train)
r2.tree.train <- rsquared(pred.tree.train)

tree.pred.test <- predict(treet.model, test)
mse.tree.test <- MSE(tree.pred.test)
r2.tree.test <- rsquared(tree.pred.test)
print(r2.tree.test)
```


#2. Random Forest 
```{r}
rf.model <- randomForest(totalinsurancepremiumofthepolicy_log ~., data = train, mtry = 3, importance = TRUE)

pred.rf.train <- predict(rf.model, train)
mse.rf.train <- MSE(pred.rf.train)
r2.rf.train <- rsquared(pred.rf.train)

rf.pred.test <- predict(rf.model, test)
mse.rf.test <- MSE(rf.pred.test)
r2.rf.test <- rsquared(rf.pred.test)
print(r2.rf.test)
```


```{r}
imp <- data.frame(importance(rf.model, type = 1))
imp <- rownames_to_column(imp, var = "variable")

ggplot(imp, aes(x = reorder(variable, X.IncMSE), y = X.IncMSE, color = reorder(variable, X.IncMSE))) +
  geom_point(show.legend=FALSE, size=3) +
  geom_segment(aes(x=variable, xend=variable, y=0, yend = X.IncMSE), size=3, show.legend=FALSE) +
  xlab("") +
  ylab("% Increase in MSE") +
  labs(title = "Variable Importance for Prediction of Total Insurance Premium") +
  coord_flip() +
  theme_classic()

dev.copy(jpeg, filename = "Random Forest.jpg")
dev.off ()
```
# PLOT ALL THE MODELS


```{r}
p1 <- ggplot(mapping = aes(x = test$totalinsurancepremiumofthepolicy_log, y = fwd.pred.test)) +
  geom_point(color = "blue") +
  geom_abline(slope = 1) +
  xlab("Predicted Total Insurance Premium") +
  ylab("Actual Insurance Premium") +
  labs(title = "Forward Direction")

p2 <- ggplot(mapping = aes(x = test$totalinsurancepremiumofthepolicy_log, y = both.pred.test)) +
  geom_point(color = "red") +
  geom_abline(slope = 1) +
  xlab("Predicted Total Insurance Premium") +
  ylab("Actual Insurance Premium") +
  labs(title = "Both Directions")

p3 <- ggplot(mapping = aes(x = test$totalinsurancepremiumofthepolicy_log, y = ridge.pred.test)) +
  geom_point(color = "purple") +
  geom_abline(slope = 1) +
  xlab("Predicted Total Insurance Premium") +
  ylab("Actual Insurance Premium") +
  labs(title = "Ridge")

p4 <- ggplot(mapping = aes(x = test$totalinsurancepremiumofthepolicy_log, y = lasso.pred.test)) +
  geom_point(color = "green") +
  geom_abline(slope = 1) +
  xlab("Predicted Total Insurance Premium") +
  ylab("Actual Insurance Premium") +
  labs(title = "Lasso")

p5 <- ggplot(mapping = aes(x = test$totalinsurancepremiumofthepolicy_log, y = tree.pred.test)) +
  geom_point(color = "black") +
  geom_abline(slope = 1) +
  xlab("Predicted Total Insurance Premium") +
  ylab("Actual Insurance Premium") +
  labs(title = "Single Tree")

p6 <- ggplot(mapping = aes(x = test$totalinsurancepremiumofthepolicy_log, y = rf.pred.test)) +
  geom_point(color = "purple") +
  geom_abline(slope = 1) +
  xlab("Predicted Total Insurance Premium") +
  ylab("Actual Insurance Premium") +
  labs(title = "Random Forest")

grid.arrange(p1, p2, p3, p4, p5, p6, nrow = 3)

dev.copy(jpeg, filename = "All Plots.jpg")
dev.off ()

```























